{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача: запустить модель LDA и Gibbs Sampling с числов тегов 20. Вывести топ-10 слов по каждому тегу. Соотнести полученные теги с тегами из датасета, сделать выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Soft\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS, analyzer='word', binary=True, min_df=0.2, max_df=1.0)\n",
    "popular = vectorizer.fit_transform(newsgroups_train.data)\n",
    "print(popular.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'know': 2, 'don': 0, 'like': 3, 'just': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывел самые \"популярные слова\" (ничего удивительного, однако не совсем понятно что за слово \"don\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем два исследования, для часто используемых слов и нет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 451)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS, analyzer='word', binary=True, min_df=0.02, max_df=1.0)\n",
    "train1 = vectorizer.fit_transform(newsgroups_train.data)\n",
    "print(train1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 3694)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS, analyzer='word', binary=True, min_df=0.001, max_df=0.002)\n",
    "train2 = vectorizer.fit_transform(newsgroups_train.data)\n",
    "print(train2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc, word = train1.nonzero()\n",
    "z = np.random.choice(20, len(doc))\n",
    "alpha = 2 * np.ones(20)\n",
    "beta = 2 * np.ones(train1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass1 = np.zeros(20)\n",
    "mass2 = np.zeros(20 * train1.shape[0]).reshape(train1.shape[0], 20)\n",
    "mass3 = np.zeros(20 * train1.shape[1]).reshape(20, train1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j, k in zip(doc, word, z):\n",
    "    mass1[k] += 1\n",
    "    mass2[i, k] += 1\n",
    "    mass3[k, j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(mass1, mass2, mass3, z, doc, word, alpha, beta):    \n",
    "    for i in tqdm(range(50)):\n",
    "        for j in range(len(doc)):\n",
    "            \n",
    "            topic1 = z[j]\n",
    "            doc1 = doc[j]\n",
    "            word1 = word[j]\n",
    "            \n",
    "            mass1[topic1] -= 1\n",
    "            mass2[doc1, topic1] -= 1\n",
    "            mass3[topic1, word1] -= 1\n",
    "            \n",
    "            p = (mass2[doc1, :] + alpha) * (mass3[:, word1] + beta[word1]) / (mass1 + beta.sum())\n",
    "            z[j] = np.random.choice(np.arange(20), p = p / p.sum())\n",
    "            \n",
    "            mass1[z[j]] += 1\n",
    "            mass2[doc1, z[j]] += 1\n",
    "            mass3[z[j], word1] += 1\n",
    "\n",
    "    return mass1, mass2, mass3, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [13:38<00:00, 16.38s/it]\n"
     ]
    }
   ],
   "source": [
    "mass1, mass2, mass3, z = LDA(mass1, mass2, mass3, z, doc, word, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "believe\t christian\t did\t don\t god\t just\t like\t people\t say\t think\n",
      "don\t good\t just\t know\t like\t lot\t really\t think\t ve\t want\n",
      "card\t dos\t file\t files\t mail\t pc\t program\t software\t thanks\t windows\n",
      "10\t 11\t 12\t 14\t 15\t 16\t 1993\t 20\t 25\t 30\n",
      "best\t going\t good\t ll\t make\t real\t think\t ve\t want\t way\n",
      "data\t general\t help\t know\t new\t number\t possible\t time\t use\t used\n",
      "don\t got\t just\t know\t need\t new\t non\t old\t time\t used\n",
      "day\t don\t government\t just\t know\t little\t ll\t new\t people\t saw\n",
      "case\t come\t did\t people\t really\t said\t say\t think\t time\t world\n",
      "bit\t does\t don\t good\t just\t like\t people\t right\t think\t time\n",
      "course\t didn\t don\t going\t like\t really\t say\t sure\t think\t ve\n",
      "does\t don\t just\t point\t problem\t read\t time\t want\t way\t work\n",
      "did\t does\t don\t know\t need\t people\t point\t probably\t right\t thing\n",
      "drive\t got\t hard\t just\t know\t like\t make\t problem\t use\t used\n",
      "case\t didn\t don\t just\t like\t tell\t think\t trying\t want\t years\n",
      "actually\t does\t make\t thing\t think\t time\t use\t ve\t way\t work\n",
      "does\t doesn\t don\t like\t line\t make\t possible\t time\t used\t using\n",
      "does\t don\t know\t like\t make\t need\t problem\t thanks\t use\t work\n",
      "does\t don\t edu\t good\t just\t know\t like\t new\t thanks\t want\n",
      "did\t don\t just\t know\t like\t people\t said\t think\t time\t way\n"
     ]
    }
   ],
   "source": [
    "answer = np.argsort(mass3, axis=1)[:, -10:]\n",
    "for i in range(20):\n",
    "    matrix = np.zeros((1, train1.shape[1]))\n",
    "    for j in answer[i]:\n",
    "        matrix[0, j] = 1\n",
    "    print('\\t '.join(vectorizer.inverse_transform(matrix)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc, word = train2.nonzero()\n",
    "z = np.random.choice(20, len(doc))\n",
    "alpha = 2 * np.ones(20)\n",
    "beta = 2 * np.ones(train2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass1 = np.zeros(20)\n",
    "mass2 = np.zeros(20 * train2.shape[0]).reshape(train2.shape[0], 20)\n",
    "mass3 = np.zeros(20 * train2.shape[1]).reshape(20, train2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j, k in zip(doc, word, z):\n",
    "    mass1[k] += 1\n",
    "    mass2[i, k] += 1\n",
    "    mass3[k, j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [04:08<00:00,  4.97s/it]\n"
     ]
    }
   ],
   "source": [
    "mass1, mass2, mass3, z = LDA(mass1, mass2, mass3, z, doc, word, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\t booted\t buddy\t bure\t fundamentalist\t loops\t overtime\t proposing\t sand\t subset\n",
      "331\t acknowledged\t cabin\t consumers\t dartmouth\t dev\t franchise\t manhattan\t photographs\t que\n",
      "263\t apostles\t danny\t dozens\t forwarded\t illusion\t netnews\t ott\t provisions\t rice\n",
      "222\t brightness\t eaten\t eyewitness\t moto\t pcx\t royals\t slmr\t specifics\t wash\n",
      "billions\t contention\t infinity\t kirk\t roster\t rotate\t seals\t smell\t ulf\t unlimited\n",
      "accompanied\t davidian\t dies\t fallacy\t markets\t parked\t stewart\t thereof\t titled\t vesselin\n",
      "attractive\t boring\t italian\t kicking\t mis\t outstanding\t paranoia\t pet\t tolerance\t tue\n",
      "625\t accessible\t bury\t demanding\t espionage\t logically\t precise\t regulars\t struggling\t understandable\n",
      "116\t abused\t documentary\t lacks\t liberals\t salary\t slaves\t sounded\t thompson\t underground\n",
      "beware\t contend\t corpses\t denies\t existent\t irrational\t jsc\t mice\t originated\t spokesman\n",
      "acute\t conscious\t copyrighted\t drum\t lamb\t magnum\t moments\t nearest\t objections\t republicans\n",
      "chelios\t col\t elementary\t freeware\t measurements\t mobile\t neck\t organize\t photographs\t quarterly\n",
      "1t\t 6e\t 9v\t ax\t d9\t ei\t gk\t m4\t tg\t tq\n",
      "explosion\t introduce\t lengthy\t progressive\t prospect\t rank\t semitic\t sharks\t ton\t unclear\n",
      "attributed\t autoexec\t bicycle\t characterize\t circles\t deluxe\t mom\t shore\t silence\t survived\n",
      "acceleration\t chase\t cute\t kits\t mandate\t mot\t pagan\t resolved\t turkiye\t variations\n",
      "arise\t attitudes\t baku\t concentration\t glenn\t jacket\t sanderson\t sleeping\t stefan\t volumes\n",
      "9000\t cosmic\t failures\t matrix\t myers\t orbiter\t performs\t rutgers\t umich\t virgin\n",
      "156\t 174\t abraham\t bags\t compromised\t grounded\t overhead\t racism\t ta\t undefined\n",
      "alabama\t chamber\t improvements\t inserted\t invitation\t kkeller\t massacred\t patrol\t robin\t spectacular\n"
     ]
    }
   ],
   "source": [
    "answer = np.argsort(mass3, axis=1)[:, -10:]\n",
    "for i in range(20):\n",
    "    matrix = np.zeros((1, train2.shape[1]))\n",
    "    for j in answer[i]:\n",
    "        matrix[0, j] = 1\n",
    "    print('\\t '.join(vectorizer.inverse_transform(matrix)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из этих двух таблиц следует довольно очевидные выводы:\n",
    "\n",
    "первая соответствует наиболее популярным словам, поэтому в каждой строке мы видим примерно одинаковые \"общие\" слова, из-за этого сложно определить к какому тегу соотносится каждая строка\n",
    "\n",
    "вторая соответствует довольно непопулярным словам, однако из-за этого иногда проскакивают числа, которые в себе никакой содержательной информации о тегах не несут, но тем не менее, из-за более специализированных \"необщих\" слов, соотнести теги гораздо проще"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь же не будем обращаться к крайностям, и попытаемся подобрать такие параметры, чтобы по словам можно было восстановить тег:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 2330)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS, analyzer='word', binary=True, min_df=0.005, max_df=0.05)\n",
    "train3 = vectorizer.fit_transform(newsgroups_train.data)\n",
    "print(train3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc, word = train3.nonzero()\n",
    "z = np.random.choice(20, len(doc))\n",
    "alpha = 2 * np.ones(20)\n",
    "beta = 2 * np.ones(train3.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass1 = np.zeros(20)\n",
    "mass2 = np.zeros(20 * train3.shape[0]).reshape(train3.shape[0], 20)\n",
    "mass3 = np.zeros(20 * train3.shape[1]).reshape(20, train3.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j, k in zip(doc, word, z):\n",
    "    mass1[k] += 1\n",
    "    mass2[i, k] += 1\n",
    "    mass3[k, j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [22:03<00:00, 26.47s/it]\n"
     ]
    }
   ],
   "source": [
    "mass1, mass2, mass3, z = LDA(mass1, mass2, mass3, z, doc, word, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag 1 \t aren\tdeal\tidea\timportant\tisn\tlikely\treason\tseen\ttimes\twrong\n",
      "Tag 2 \t card\tcomputer\tdisk\tdos\tdrive\tmac\tmemory\tpc\tsoftware\tvideo\n",
      "Tag 3 \t ask\tgroup\tisn\tkind\tmean\tplace\tquite\tseen\tsort\twrong\n",
      "Tag 4 \t guess\thand\tidea\tkind\tmakes\tmeans\tperson\tpretty\tthought\twrong\n",
      "Tag 5 \t 11\t12\t14\t15\t16\t20\t24\t25\t30\tteam\n",
      "Tag 6 \t bible\tchrist\tchristian\tchristians\tclaim\tfaith\tjesus\tlife\treligion\tword\n",
      "Tag 7 \t application\tcode\temail\tfile\tfiles\tlist\tprogram\tversion\twindow\tworks\n",
      "Tag 8 \t bad\tbuy\tgets\tgoes\tknown\tlarge\tline\tlooks\ttimes\tyes\n",
      "Tag 9 \t ago\tbig\tchange\tmean\tmonths\tnear\tnote\tsmall\ttimes\twon\n",
      "Tag 10 \t big\tcouldn\tgroup\tlow\torder\tpretty\tsupport\ttalk\ttried\tworking\n",
      "Tag 11 \t car\tcontrol\tfree\tinfo\tinstead\tline\torder\tpretty\tseen\twon\n",
      "Tag 12 \t article\tcommon\tfast\tgetting\tgiven\theard\tline\tmatter\tstate\tsupport\n",
      "Tag 13 \t ago\tbanks\tchastity\tdsl\tgordon\tintellect\tpitt\tskepticism\tsoon\tsurrender\n",
      "Tag 14 \t advance\tanybody\tdoing\tfree\tinterested\tkind\tlocal\tsend\tstuff\tyes\n",
      "Tag 15 \t bad\tcost\tcouple\tposting\tproblems\tquite\tseen\tstart\ttold\twon\n",
      "Tag 16 \t 1993\tdata\tencryption\tgovernment\tkey\tmessage\tphone\tpublic\tsystems\ttechnology\n",
      "Tag 17 \t bad\tbased\texample\theard\thi\tinterested\tisn\tpretty\tstart\ttrying\n",
      "Tag 18 \t big\tgetting\tleft\tsays\tsimilar\tsmall\tspace\tstuff\tthought\tyes\n",
      "Tag 19 \t away\tchildren\tgovernment\tisrael\tkilled\trights\tstate\tstates\ttoday\twar\n",
      "Tag 20 \t agree\tblack\tbook\tchange\tcost\tkind\tquite\treason\ttype\tyes\n"
     ]
    }
   ],
   "source": [
    "answer = np.argsort(mass3, axis=1)[:, -10:]\n",
    "for i in range(20):\n",
    "    matrix = np.zeros((1, train3.shape[1]))\n",
    "    for j in answer[i]:\n",
    "        matrix[0, j] = 1\n",
    "    print('Tag', i + 1,'\\t', '\\t'.join(vectorizer.inverse_transform(matrix)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По этим словам уже можно определять некоторые теги наверняка:\n",
    "\n",
    "2) - comp.sys.ibm.pc.hardware\n",
    "\n",
    "5), 9) - rec.sport.baseball || rec.sport.hockey\n",
    "\n",
    "6) - soc.religion.christian\n",
    "\n",
    "7) - comp.os.ms-windows.misc\n",
    "\n",
    "11) - rec.motorcycles || rec.autos\n",
    "\n",
    "16) - sci.crypt\n",
    "\n",
    "19) - talk.politics.mideast\n",
    "\n",
    "Также можно сделать выводы о тегах 8, 14, 18, 20: во всех четырех присутствует слово \"yes\", которое употребляется только в разговорном стиле, то есть все эти четыре тега соответствуют одному из четырех пунктов списка ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставшиеся теги определить досаточно сложно, но можно руководствоваться остаточным принципом, и раскидать оставшиеся теги, как например: 13) - alt.atheism, 1) - misc.forsale и так далее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
